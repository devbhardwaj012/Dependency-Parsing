{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n========================================================================\nGRAPH-BASED DEPENDENCY PARSING PROJECT\nMaximum Spanning Tree (MST) Algorithm Implementation\n========================================================================\n\nProject Overview:\nThis project implements a classical graph-based dependency parser using\nthe Chu-Liu-Edmonds algorithm for finding Maximum Spanning Trees. It\ndemonstrates how dependency parsing was approached before neural methods.\n\nKey Components:\n- Chu-Liu-Edmonds MST algorithm\n- Structured Perceptron training\n- Feature-based scoring system\n- Comprehensive evaluation and visualization\n\nDataset: Universal Dependencies English Treebank (UD_English-EWT)\nInstall: pip install conllu numpy scipy scikit-learn matplotlib\n\"\"\"\n\nimport numpy as np\nfrom collections import defaultdict, Counter\nimport re\nfrom typing import List, Tuple, Dict\nimport pickle\nimport time\nfrom datetime import datetime\n\n# For Kaggle: Uncomment to download UD English Treebank\n# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\n# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\n# !wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\n\nclass DependencyTree:\n    \"\"\"Represents a dependency tree structure\"\"\"\n    def __init__(self, tokens, heads, labels):\n        self.tokens = tokens\n        self.heads = heads\n        self.labels = labels\n        \n    def __repr__(self):\n        result = []\n        for i, (token, head, label) in enumerate(zip(self.tokens, self.heads, self.labels)):\n            result.append(f\"{i}: {token} <- {head} ({label})\")\n        return \"\\n\".join(result)\n\nclass CoNLLReader:\n    \"\"\"Reads CoNLL-U format dependency treebanks\"\"\"\n    \n    @staticmethod\n    def read_conllu(filepath):\n        sentences = []\n        current_tokens = []\n        current_heads = []\n        current_labels = []\n        \n        with open(filepath, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                \n                if line.startswith('#') or not line:\n                    if current_tokens:\n                        sentences.append(DependencyTree(\n                            current_tokens, current_heads, current_labels\n                        ))\n                        current_tokens = []\n                        current_heads = []\n                        current_labels = []\n                    continue\n                \n                parts = line.split('\\t')\n                if len(parts) >= 8 and '-' not in parts[0] and '.' not in parts[0]:\n                    token = parts[1]\n                    head = int(parts[6])\n                    label = parts[7]\n                    \n                    current_tokens.append(token)\n                    current_heads.append(head)\n                    current_labels.append(label)\n        \n        if current_tokens:\n            sentences.append(DependencyTree(\n                current_tokens, current_heads, current_labels\n            ))\n        \n        return sentences\n\nclass FeatureExtractor:\n    \"\"\"Extracts features for scoring dependency arcs\"\"\"\n    \n    def __init__(self):\n        self.feature_weights = defaultdict(float)\n        self.feature_counts = Counter()\n        \n    def extract_features(self, tokens, head_idx, dep_idx):\n        \"\"\"Extract features for a potential dependency arc\"\"\"\n        features = []\n        \n        # Handle ROOT and ensure indices are valid\n        if head_idx == 0:\n            head_token = \"ROOT\"\n        elif head_idx > 0 and head_idx <= len(tokens):\n            head_token = tokens[head_idx - 1]\n        else:\n            head_token = \"UNK\"\n        \n        if dep_idx >= 0 and dep_idx < len(tokens):\n            dep_token = tokens[dep_idx]\n        else:\n            dep_token = \"UNK\"\n        \n        # Distance features\n        distance = abs(head_idx - dep_idx)\n        features.append(f\"dist={distance}\")\n        features.append(f\"dist_bin={min(distance, 5)}\")\n        \n        # Direction feature\n        direction = \"right\" if dep_idx > head_idx else \"left\"\n        features.append(f\"dir={direction}\")\n        \n        # Lexical features\n        features.append(f\"head={head_token.lower()}\")\n        features.append(f\"dep={dep_token.lower()}\")\n        features.append(f\"head_dep={head_token.lower()}_{dep_token.lower()}\")\n        \n        # Suffix features\n        if len(dep_token) >= 3:\n            features.append(f\"dep_suffix={dep_token[-3:].lower()}\")\n        if len(head_token) >= 3 and head_idx > 0:\n            features.append(f\"head_suffix={head_token[-3:].lower()}\")\n        \n        # Capitalization features\n        features.append(f\"dep_cap={dep_token[0].isupper()}\")\n        \n        # Position features\n        if dep_idx == 0:\n            features.append(\"dep_is_first\")\n        if dep_idx == len(tokens) - 1:\n            features.append(\"dep_is_last\")\n        \n        return features\n    \n    def score_arc(self, tokens, head_idx, dep_idx):\n        \"\"\"Score a potential dependency arc\"\"\"\n        features = self.extract_features(tokens, head_idx, dep_idx)\n        score = sum(self.feature_weights[f] for f in features)\n        return score\n\nclass ChuLiuEdmonds:\n    \"\"\"Implementation of Chu-Liu-Edmonds algorithm for finding MST\"\"\"\n    \n    @staticmethod\n    def find_mst(score_matrix):\n        \"\"\"Find maximum spanning tree using Chu-Liu-Edmonds algorithm\"\"\"\n        n = len(score_matrix)\n        \n        best_heads = [0] * n\n        for j in range(1, n):\n            best_score = float('-inf')\n            best_head = 0\n            for i in range(n):\n                if i != j and score_matrix[i][j] > best_score:\n                    best_score = score_matrix[i][j]\n                    best_head = i\n            best_heads[j] = best_head\n        \n        visited = [False] * n\n        cycle = ChuLiuEdmonds._find_cycle(best_heads, visited)\n        \n        if cycle is None:\n            return best_heads\n        \n        return ChuLiuEdmonds._contract_cycle(score_matrix, best_heads, cycle)\n    \n    @staticmethod\n    def _find_cycle(heads, visited):\n        \"\"\"Detect cycle in the current tree\"\"\"\n        n = len(heads)\n        for start in range(1, n):\n            if visited[start]:\n                continue\n            \n            path = []\n            current = start\n            \n            while current not in path and not visited[current]:\n                path.append(current)\n                current = heads[current]\n                if current == 0:\n                    break\n            \n            if current in path:\n                cycle_start = path.index(current)\n                return path[cycle_start:]\n            \n            for node in path:\n                visited[node] = True\n        \n        return None\n    \n    @staticmethod\n    def _contract_cycle(score_matrix, heads, cycle):\n        \"\"\"Contract a cycle and recursively find MST\"\"\"\n        n = len(score_matrix)\n        cycle_set = set(cycle)\n        \n        non_cycle = [i for i in range(n) if i not in cycle_set]\n        new_node_idx = len(non_cycle)\n        new_size = len(non_cycle) + 1\n        \n        new_score_matrix = [[float('-inf')] * new_size for _ in range(new_size)]\n        \n        old_to_new = {}\n        for idx, node in enumerate(non_cycle):\n            old_to_new[node] = idx\n        for node in cycle:\n            old_to_new[node] = new_node_idx\n        \n        for i in range(n):\n            for j in range(n):\n                if i == j:\n                    continue\n                new_i = old_to_new[i]\n                new_j = old_to_new[j]\n                \n                if new_i == new_j:\n                    continue\n                \n                score = score_matrix[i][j]\n                \n                if j in cycle_set and i not in cycle_set:\n                    score -= score_matrix[heads[j]][j]\n                \n                new_score_matrix[new_i][new_j] = max(\n                    new_score_matrix[new_i][new_j], score\n                )\n        \n        new_heads = ChuLiuEdmonds.find_mst(new_score_matrix)\n        \n        final_heads = list(heads)\n        \n        for node in cycle:\n            if old_to_new[heads[node]] != new_node_idx:\n                pass\n            elif new_heads[new_node_idx] != new_node_idx:\n                external_head = non_cycle[new_heads[new_node_idx]] if new_heads[new_node_idx] < new_node_idx else None\n                if external_head is not None:\n                    final_heads[node] = external_head\n                    break\n        \n        return final_heads\n\nclass MSTParser:\n    \"\"\"Graph-based dependency parser using MST algorithm\"\"\"\n    \n    def __init__(self):\n        self.feature_extractor = FeatureExtractor()\n        self.training_stats = {\n            'epoch_accuracies': [],\n            'epoch_times': [],\n            'total_updates': 0\n        }\n        \n    def train(self, train_trees, epochs=10, learning_rate=0.1, verbose=True):\n        \"\"\"Train the parser using structured perceptron\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"TRAINING PHASE\")\n        print(\"=\"*80)\n        print(f\"Training sentences: {len(train_trees)}\")\n        print(f\"Epochs: {epochs}\")\n        print(f\"Learning rate: {learning_rate}\")\n        print(f\"Algorithm: Structured Perceptron with MST inference\")\n        print(\"=\"*80 + \"\\n\")\n        \n        for epoch in range(epochs):\n            epoch_start = time.time()\n            correct_arcs = 0\n            total_arcs = 0\n            updates = 0\n            \n            for tree_idx, gold_tree in enumerate(train_trees):\n                if tree_idx % 100 == 0 and verbose:\n                    print(f\"Epoch {epoch+1}/{epochs} | Sentence {tree_idx}/{len(train_trees)} | \"\n                          f\"Updates: {updates}\", end='\\r')\n                \n                predicted_heads = self.parse(gold_tree.tokens)\n                \n                if len(predicted_heads) != len(gold_tree.tokens):\n                    continue\n                \n                n = len(gold_tree.tokens)\n                for dep_idx in range(n):\n                    gold_head = gold_tree.heads[dep_idx]\n                    pred_head = predicted_heads[dep_idx]\n                    \n                    if pred_head < 0 or pred_head > n:\n                        continue\n                    \n                    if gold_head == pred_head:\n                        correct_arcs += 1\n                    else:\n                        # Update for gold arc\n                        gold_features = self.feature_extractor.extract_features(\n                            gold_tree.tokens, gold_head, dep_idx\n                        )\n                        for feat in gold_features:\n                            self.feature_extractor.feature_weights[feat] += learning_rate\n                            self.feature_extractor.feature_counts[feat] += 1\n                        \n                        # Update for predicted arc\n                        pred_features = self.feature_extractor.extract_features(\n                            gold_tree.tokens, pred_head, dep_idx\n                        )\n                        for feat in pred_features:\n                            self.feature_extractor.feature_weights[feat] -= learning_rate\n                            self.feature_extractor.feature_counts[feat] += 1\n                        \n                        updates += 1\n                    \n                    total_arcs += 1\n            \n            epoch_time = time.time() - epoch_start\n            accuracy = correct_arcs / total_arcs if total_arcs > 0 else 0\n            \n            self.training_stats['epoch_accuracies'].append(accuracy)\n            self.training_stats['epoch_times'].append(epoch_time)\n            self.training_stats['total_updates'] += updates\n            \n            print(f\"\\nEpoch {epoch+1}/{epochs} Complete:\")\n            print(f\"  ├─ Accuracy: {accuracy:.4f} ({correct_arcs}/{total_arcs})\")\n            print(f\"  ├─ Updates: {updates}\")\n            print(f\"  ├─ Time: {epoch_time:.2f}s\")\n            print(f\"  └─ Sentences/sec: {len(train_trees)/epoch_time:.1f}\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"TRAINING COMPLETE\")\n        print(\"=\"*80)\n        print(f\"Total updates: {self.training_stats['total_updates']}\")\n        print(f\"Active features: {len(self.feature_extractor.feature_weights)}\")\n        print(f\"Final accuracy: {self.training_stats['epoch_accuracies'][-1]:.4f}\")\n        print(\"=\"*80 + \"\\n\")\n        \n    def parse(self, tokens):\n        \"\"\"Parse a sentence and return head indices\"\"\"\n        n = len(tokens)\n        score_matrix = [[float('-inf')] * (n + 1) for _ in range(n + 1)]\n        \n        for j in range(1, n + 1):\n            score_matrix[0][j] = self.feature_extractor.score_arc(tokens, 0, j - 1)\n        \n        for i in range(1, n + 1):\n            for j in range(1, n + 1):\n                if i != j:\n                    score_matrix[i][j] = self.feature_extractor.score_arc(tokens, i, j - 1)\n        \n        heads = ChuLiuEdmonds.find_mst(score_matrix)\n        \n        token_heads = []\n        for i in range(1, n + 1):\n            head = heads[i]\n            if head < 0 or head > n:\n                head = 0\n            token_heads.append(head)\n        \n        return token_heads\n    \n    def evaluate(self, test_trees, detailed=True):\n        \"\"\"Evaluate parser on test set with detailed metrics\"\"\"\n        correct = 0\n        total = 0\n        error_types = Counter()\n        distance_errors = []\n        \n        for tree in test_trees:\n            predicted_heads = self.parse(tree.tokens)\n            for i, (pred_head, gold_head) in enumerate(zip(predicted_heads, tree.heads)):\n                if pred_head == gold_head:\n                    correct += 1\n                else:\n                    # Categorize error\n                    gold_dist = abs(gold_head - (i + 1))\n                    pred_dist = abs(pred_head - (i + 1))\n                    distance_errors.append((gold_dist, pred_dist))\n                    \n                    if gold_head == 0:\n                        error_types['wrong_root'] += 1\n                    elif pred_head == 0:\n                        error_types['false_root'] += 1\n                    else:\n                        error_types['wrong_attachment'] += 1\n                \n                total += 1\n        \n        uas = correct / total if total > 0 else 0\n        \n        if detailed:\n            print(f\"\\nUnlabeled Attachment Score (UAS): {uas:.4f}\")\n            print(f\"Correct attachments: {correct}/{total}\")\n            print(f\"\\nError Breakdown:\")\n            print(f\"  ├─ Wrong root: {error_types['wrong_root']}\")\n            print(f\"  ├─ False root: {error_types['false_root']}\")\n            print(f\"  └─ Wrong attachment: {error_types['wrong_attachment']}\")\n        \n        return uas, error_types\n    \n    def analyze_features(self, top_k=20):\n        \"\"\"Analyze learned feature weights\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"FEATURE ANALYSIS\")\n        print(\"=\"*80)\n        \n        sorted_features = sorted(self.feature_extractor.feature_weights.items(), \n                                key=lambda x: abs(x[1]), reverse=True)\n        \n        print(f\"\\nTop {top_k} Most Important Features:\")\n        print(\"-\" * 80)\n        print(f\"{'Feature':<50} {'Weight':>15} {'Count':>10}\")\n        print(\"-\" * 80)\n        \n        for feat, weight in sorted_features[:top_k]:\n            count = self.feature_extractor.feature_counts[feat]\n            print(f\"{feat:<50} {weight:>15.4f} {count:>10}\")\n        \n        print(\"\\n\" + \"=\"*80 + \"\\n\")\n    \n    def save(self, filepath):\n        \"\"\"Save model to file\"\"\"\n        with open(filepath, 'wb') as f:\n            pickle.dump({\n                'weights': self.feature_extractor.feature_weights,\n                'stats': self.training_stats\n            }, f)\n        print(f\"Model saved to {filepath}\")\n    \n    def load(self, filepath):\n        \"\"\"Load model from file\"\"\"\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n            self.feature_extractor.feature_weights = data['weights']\n            self.training_stats = data['stats']\n        print(f\"Model loaded from {filepath}\")\n\ndef visualize_parse_tree(tokens, heads, labels=None):\n    \"\"\"Create ASCII art visualization of dependency tree\"\"\"\n    n = len(tokens)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"DEPENDENCY TREE VISUALIZATION\")\n    print(\"=\"*80)\n    \n    # Print tokens with indices\n    print(\"\\nTokens:\")\n    for i, token in enumerate(tokens):\n        print(f\"  [{i+1}] {token}\")\n    \n    print(\"\\nDependency Arcs:\")\n    print(\"-\" * 80)\n    print(f\"{'Dependent':<15} {'Head':<15} {'Relation':<15} {'Direction':<10}\")\n    print(\"-\" * 80)\n    \n    for i, (token, head) in enumerate(zip(tokens, heads)):\n        head_token = \"ROOT\" if head == 0 else tokens[head - 1]\n        label = labels[i] if labels else \"dep\"\n        direction = \"←\" if head < i + 1 else \"→\" if head > i + 1 else \"↓\"\n        \n        print(f\"{token:<15} {head_token:<15} {label:<15} {direction:<10}\")\n    \n    print(\"=\"*80 + \"\\n\")\n\ndef test_on_diverse_examples(parser):\n    \"\"\"Comprehensive testing on diverse linguistic phenomena\"\"\"\n    \n    test_cases = [\n        {\n            'category': 'Simple SVO',\n            'sentences': [\n                [\"I\", \"saw\", \"her\"],\n                [\"Dogs\", \"bark\", \"loudly\"],\n                [\"The\", \"cat\", \"sleeps\"],\n            ]\n        },\n        {\n            'category': 'Prepositional Phrases (PP-Attachment)',\n            'sentences': [\n                [\"I\", \"saw\", \"the\", \"man\", \"with\", \"the\", \"telescope\"],\n                [\"She\", \"put\", \"the\", \"book\", \"on\", \"the\", \"table\"],\n            ]\n        },\n        {\n            'category': 'Relative Clauses',\n            'sentences': [\n                [\"The\", \"dog\", \"that\", \"I\", \"saw\", \"was\", \"big\"],\n                [\"People\", \"who\", \"live\", \"here\", \"are\", \"happy\"],\n            ]\n        },\n        {\n            'category': 'Coordination',\n            'sentences': [\n                [\"John\", \"and\", \"Mary\", \"went\", \"home\"],\n                [\"She\", \"quickly\", \"and\", \"quietly\", \"left\"],\n            ]\n        },\n        {\n            'category': 'Long-Distance Dependencies',\n            'sentences': [\n                [\"What\", \"did\", \"you\", \"think\", \"he\", \"said\"],\n                [\"Who\", \"do\", \"you\", \"believe\", \"won\"],\n            ]\n        },\n        {\n            'category': 'Complex Sentences',\n            'sentences': [\n                [\"I\", \"think\", \"that\", \"he\", \"is\", \"smart\"],\n                [\"The\", \"horse\", \"raced\", \"past\", \"the\", \"barn\", \"fell\"],\n            ]\n        }\n    ]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"COMPREHENSIVE TESTING ON DIVERSE LINGUISTIC PHENOMENA\")\n    print(\"=\"*80)\n    \n    for test_group in test_cases:\n        print(f\"\\n{'─'*80}\")\n        print(f\"Category: {test_group['category']}\")\n        print(f\"{'─'*80}\")\n        \n        for sent_tokens in test_group['sentences']:\n            print(f\"\\nSentence: {' '.join(sent_tokens)}\")\n            heads = parser.parse(sent_tokens)\n            \n            print(\"Parse:\")\n            for i, (token, head) in enumerate(zip(sent_tokens, heads)):\n                head_token = \"ROOT\" if head == 0 else sent_tokens[head - 1]\n                arrow = \"←\" if head < i + 1 else \"→\" if head > i + 1 else \"↓\"\n                print(f\"  {token:15s} {arrow} {head_token:15s}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"KNOWN FAILURE MODES AND LIMITATIONS\")\n    print(\"=\"*80)\n    \n    failure_analysis = \"\"\"\n    1. PP-ATTACHMENT AMBIGUITY\n       Example: \"I saw the man with the telescope\"\n       Challenge: Should 'telescope' attach to 'saw' (instrument) or 'man' (possession)?\n       Why it fails: Requires semantic knowledge about plausibility\n       \n    2. LONG-DISTANCE DEPENDENCIES\n       Example: \"What did you think that he said\"\n       Challenge: 'What' is the object of 'said', several words away\n       Why it fails: Non-local dependencies hard to capture with local features\n       \n    3. COORDINATION AMBIGUITY\n       Example: \"old men and women\"\n       Challenge: Does 'old' modify just 'men' or both 'men and women'?\n       Why it fails: Requires understanding of scope and semantic parallelism\n       \n    4. GARDEN PATH SENTENCES\n       Example: \"The horse raced past the barn fell\"\n       Challenge: 'raced' is initially parsed as main verb, but is actually reduced relative\n       Why it fails: No reanalysis mechanism; commits to first interpretation\n       \n    5. LIMITED LEXICAL SEMANTICS\n       Example: Semantic role assignment, verb subcategorization\n       Challenge: Different verbs have different argument structures\n       Why it fails: No access to lexical semantic databases or embeddings\n       \n    IMPROVEMENTS THAT WOULD HELP:\n    - Rich POS tags and morphological features\n    - Word embeddings or distributional semantics\n    - Syntactic category information\n    - Lexicalized grammar knowledge\n    - Larger context windows\n    - Neural scoring functions\n    \"\"\"\n    \n    print(failure_analysis)\n    print(\"=\"*80 + \"\\n\")\n    \n    # Add detailed comparison with modern approaches\n    print_where_classical_models_lag()\n\ndef generate_project_report(parser, train_trees, dev_trees, test_trees):\n    \"\"\"Generate comprehensive project report\"\"\"\n    \n    print(\"\\n\" + \"╔\" + \"=\"*78 + \"╗\")\n    print(\"║\" + \" \"*15 + \"GRAPH-BASED DEPENDENCY PARSING PROJECT REPORT\" + \" \"*16 + \"║\")\n    print(\"╚\" + \"=\"*78 + \"╝\")\n    \n    print(f\"\\nProject Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Algorithm: Maximum Spanning Tree (Chu-Liu-Edmonds)\")\n    print(f\"Training Method: Structured Perceptron\")\n    \n    print(\"\\n\" + \"─\"*80)\n    print(\"DATASET STATISTICS\")\n    print(\"─\"*80)\n    \n    print(f\"Training sentences: {len(train_trees)}\")\n    print(f\"Development sentences: {len(dev_trees)}\")\n    print(f\"Test sentences: {len(test_trees)}\")\n    \n    train_tokens = sum(len(tree.tokens) for tree in train_trees)\n    print(f\"Training tokens: {train_tokens}\")\n    print(f\"Average sentence length: {train_tokens/len(train_trees):.1f} tokens\")\n    \n    # Analyze dependency label distribution\n    label_counts = Counter()\n    for tree in train_trees:\n        label_counts.update(tree.labels)\n    \n    print(f\"\\nMost common dependency relations:\")\n    for label, count in label_counts.most_common(10):\n        print(f\"  {label:<15} {count:>6} ({count/sum(label_counts.values())*100:.1f}%)\")\n    \n    print(\"\\n\" + \"─\"*80)\n    print(\"MODEL STATISTICS\")\n    print(\"─\"*80)\n    \n    print(f\"Total features learned: {len(parser.feature_extractor.feature_weights)}\")\n    print(f\"Total weight updates: {parser.training_stats['total_updates']}\")\n    print(f\"Training epochs: {len(parser.training_stats['epoch_accuracies'])}\")\n    \n    # Feature analysis\n    parser.analyze_features(top_k=15)\n    \n    print(\"\\n\" + \"=\"*80 + \"\\n\")\n\ndef print_where_classical_models_lag():\n    \"\"\"Detailed analysis of where classical graph-based parsers lag behind modern approaches\"\"\"\n    \n    print(\"\\n\" + \"╔\" + \"=\"*78 + \"╗\")\n    print(\"║\" + \" \"*10 + \"WHERE CLASSICAL GRAPH-BASED PARSERS LAG BEHIND\" + \" \"*20 + \"║\")\n    print(\"╚\" + \"=\"*78 + \"╝\\n\")\n    \n    print(\"=\"*80)\n    print(\"1. FEATURE ENGINEERING BOTTLENECK\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach (This Model):\n  • Manual feature templates (distance, lexical, suffix features)\n  • Sparse, high-dimensional feature vectors\n  • Human expert knowledge required\n  • Cannot capture complex feature interactions\n  • Limited to predefined feature combinations\n\nModern Neural Approach:\n  • Automatic feature learning via embeddings\n  • Dense, continuous representations\n  • Captures complex non-linear patterns\n  • Pre-trained contextualized embeddings (BERT, RoBERTa)\n  • Multi-head attention captures arbitrary feature interactions\n\nPerformance Gap: \n  • Classical MST: 60-75% UAS with basic features, 85-90% with engineered features\n  • Neural (BiLSTM): 93-95% UAS\n  • Transformer-based: 96-98% UAS\n\nReal Example Failure:\n  \"I deposited the check at the bank\"\n  Classical: Likely attaches \"bank\" to \"deposited\" (correct)\n  \n  \"I sat by the bank\"  \n  Classical: May still attach \"bank\" to \"sat\" using same features\n  ❌ Cannot distinguish financial vs. river bank without semantic context\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"2. INABILITY TO CAPTURE SEMANTIC SIMILARITY\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach:\n  • Treats \"dog\" and \"cat\" as completely different symbols\n  • No notion of semantic relatedness\n  • Cannot generalize across similar words\n  • Sparse data problem: unseen word pairs\n\nModern Approach:\n  • Word embeddings place similar words nearby in vector space\n  • \"dog\" and \"cat\" have similar representations\n  • Generalizes patterns learned from \"dog\" to \"cat\"\n  • Pre-training on massive corpora provides semantic knowledge\n\nExample:\n  Training: \"The dog chased the cat\"\n  \n  Test: \"The wolf hunted the rabbit\"\n  \n  Classical: ❌ Treats \"wolf\" and \"hunted\" as completely novel\n  Neural: ✓ Recognizes similarity to \"dog\" and \"chased\"\n  \nImpact: \n  Classical models suffer 10-15% accuracy drop on out-of-vocabulary words\n  Neural models maintain 95%+ accuracy even with 20% OOV rate\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"3. LIMITED CONTEXT WINDOW\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach:\n  • Features typically look at 2-3 word window\n  • Cannot capture long-range dependencies effectively\n  • Context beyond immediate neighbors is hard to encode\n  • Exponential feature explosion with larger windows\n\nModern Approach:\n  • BiLSTMs: Can access full sentence context\n  • Transformers: Self-attention over entire sequence\n  • No limit on context size\n  • Efficiently models arbitrarily long dependencies\n\nExample Failure:\n  \"The keys to the cabinet that the man who lives downstairs owns are missing\"\n  \n  Classical: Struggles to connect \"keys\" with \"are\" (10 words apart)\n  ❌ Features can't span that distance without exponential combinations\n  \n  Neural: ✓ Attention mechanism directly connects distant words\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"4. GREEDY STRUCTURED PREDICTION\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical MST Parser:\n  • Finds globally optimal tree given arc scores\n  • BUT arc scores are independently computed\n  • Cannot model inter-arc dependencies during scoring\n  • Structured perceptron updates are post-hoc corrections\n\nModern Approaches:\n  • Biaffine attention: Jointly scores all potential arcs\n  • Graph neural networks: Iteratively refine arc scores\n  • Structured training objectives that directly optimize tree accuracy\n  • End-to-end differentiable training\n\nProblem Example:\n  In \"John and Mary went home\", if model incorrectly scores:\n    - \"John\" → ROOT (high score)\n    - \"Mary\" → ROOT (high score)\n  \n  Classical: ❌ MST must pick one, but both have high independent scores\n  No mechanism to say \"if John is root, penalize Mary being root\"\n  \n  Neural: ✓ Can learn that coordinated subjects shouldn't both be roots\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"5. NO TRANSFER LEARNING OR PRE-TRAINING\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach:\n  • Trained from scratch on labeled dependency data\n  • Requires large annotated treebanks (10,000+ sentences)\n  • Cannot leverage unlabeled text\n  • Separate models for each language\n\nModern Approach:\n  • Pre-train on billions of tokens of unlabeled text\n  • Fine-tune on small labeled datasets (1,000 sentences can suffice)\n  • Transfer learning across languages (multilingual BERT)\n  • Can leverage knowledge from related tasks\n\nImpact on Low-Resource Languages:\n  Classical: Needs 10K+ sentences → 75% UAS\n  Neural (from scratch): Same requirement → 85% UAS\n  Neural (pre-trained): Just 1K sentences → 90% UAS\n  Neural (multilingual): Even works zero-shot! → 75-80% UAS\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"6. FIXED FEATURE REPRESENTATIONS\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach:\n  • Word \"bank\" always has same features\n  • Cannot handle polysemy (words with multiple meanings)\n  • Context-independent representation\n  • Ambiguity must be resolved by parser, not representation\n\nModern Contextualized Embeddings:\n  • \"bank\" has different representations in different contexts\n  • BERT/ELMo create context-dependent embeddings\n  • Ambiguity partially resolved before parsing\n  • Richer input to parser\n\nExample:\n  \"I went to the bank to deposit money\"\n  \"I sat by the river bank\"\n  \n  Classical: \"bank\" → same features in both\n  ❌ Parser must figure out the difference from limited context features\n  \n  BERT: \"bank\" → different embeddings (financial institution vs. riverbank)\n  ✓ Pre-resolved ambiguity helps parser make correct attachment\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"7. TRAINING DATA EFFICIENCY\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Perceptron:\n  • Requires many passes over data (5-20 epochs)\n  • Slow convergence on complex patterns\n  • Prone to overfitting on rare features\n  • Hand-crafted features may not capture patterns in data\n\nModern Neural Networks:\n  • More sample efficient with pre-training\n  • Gradient-based optimization converges faster\n  • Regularization techniques (dropout, batch norm) prevent overfitting\n  • Learns optimal features from data\n\nTraining Time Comparison (to 90% UAS):\n  Classical MST: 10K sentences × 15 epochs = 150K sentence-epochs\n  Neural (scratch): 10K sentences × 30 epochs = 300K sentence-epochs\n  Neural (pre-trained): 2K sentences × 10 epochs = 20K sentence-epochs\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"8. HANDLING OF RARE PHENOMENA\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Sparse Features:\n  • Rare word pairs get few training examples\n  • Model cannot generalize from similar contexts\n  • Effectively backs off to simple heuristics\n  • Example: Rare verbs parsed using generic \"verb\" patterns\n\nNeural Dense Representations:\n  • Rare words still have meaningful embeddings\n  • Similar words provide indirect evidence\n  • Continuous space allows interpolation\n  • Compositional understanding helps with novel constructions\n\nExample:\n  Rare verb \"scrutinize\" appears once in training\n  \n  Classical: ❌ Creates features like \"scrutinize_OBJ\" with single training example\n  Cannot generalize\n  \n  Neural: ✓ \"scrutinize\" embedding is close to \"examine\", \"inspect\"\n  Learns from all similar verbs → better generalization\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"9. CROSS-LINGUAL PARSING\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Approach:\n  • Completely separate models per language\n  • Features like word forms are language-specific\n  • No knowledge transfer between languages\n  • Need large treebank for each new language\n\nModern Multilingual Models:\n  • Single model handles 100+ languages\n  • Shared multilingual embeddings (mBERT, XLM-R)\n  • Zero-shot transfer: Train on one language, parse another\n  • Low-resource languages benefit from high-resource ones\n\nZero-Shot Performance:\n  Train on English, test on Spanish:\n    Classical: 0% (no Spanish vocabulary)\n    Multilingual BERT: 75-80% UAS\n    \n  This is revolutionary for low-resource languages!\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"10. PRACTICAL DEPLOYMENT CONSIDERATIONS\")\n    print(\"=\"*80)\n    print(\"\"\"\nClassical Model Advantages:\n  ✓ Fast inference (MST algorithm is O(n²))\n  ✓ Small model size (few MB)\n  ✓ Interpretable features\n  ✓ No GPU required\n  ✓ Deterministic output\n\nClassical Model Disadvantages:\n  ❌ 10-15% lower accuracy\n  ❌ Requires linguistic expertise for feature engineering\n  ❌ Hard to improve beyond certain accuracy ceiling\n  ❌ Separate preprocessing pipeline (POS tagging, etc.)\n\nModern Neural Model Tradeoffs:\n  ✓ State-of-the-art accuracy\n  ✓ End-to-end training\n  ✓ Easily adaptable to new domains\n  \n  ❌ Slower inference (especially transformers)\n  ❌ Large model size (100MB - 1GB)\n  ❌ Requires GPU for practical speed\n  ❌ Black box (hard to interpret)\n\nUse Cases:\n  Classical: Educational, resource-constrained, need interpretability\n  Neural: Production systems where accuracy is critical\n    \"\"\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY: PERFORMANCE COMPARISON\")\n    print(\"=\"*80)\n    print(\"\"\"\nModel Type                           UAS (English)    Speed       Model Size\n─────────────────────────────────────────────────────────────────────────────\nClassical MST (basic features)          65-75%        1000 sent/s     5 MB\nClassical MST (engineered features)     85-90%        500 sent/s      50 MB\nNeural BiLSTM (no pre-train)            91-93%        100 sent/s      100 MB\nNeural BiLSTM + BiAffine                93-95%        80 sent/s       150 MB\nBERT-based (pre-trained)                96-98%        20 sent/s       400 MB\nCurrent SOTA (2024)                     98-99%        10 sent/s       1 GB\n\nKey Insight:\nThe gap from 90% → 98% accuracy required fundamentally different approaches:\n  • Continuous representations instead of sparse features\n  • Neural composition instead of manual feature templates  \n  • Pre-training instead of training from scratch\n  • End-to-end learning instead of pipeline systems\n    \"\"\")\n    \n    print(\"\\n\" + \"╔\" + \"=\"*78 + \"╗\")\n    print(\"║\" + \" \"*15 + \"CONCLUSION: WHY NEURAL PARSERS DOMINATE\" + \" \"*23 + \"║\")\n    print(\"╚\" + \"=\"*78 + \"╝\\n\")\n    \n    print(\"\"\"\nClassical graph-based parsers were a major advance in the 2000s, but they\nfundamentally cannot compete with modern neural approaches because:\n\n1. They cannot learn semantic representations from data\n2. They require manual feature engineering by experts\n3. They cannot efficiently use unlabeled data\n4. They cannot transfer knowledge across languages or domains\n5. They plateau at ~90% accuracy regardless of data size\n\nNeural parsers overcome ALL of these limitations, achieving near-human\nperformance (98-99% UAS) on standard benchmarks.\n\nHowever, classical parsers remain valuable for:\n  • Understanding the foundations of dependency parsing\n  • Educational purposes\n  • Resource-constrained environments\n  • Interpretable systems where you need to explain decisions\n  • Baseline comparisons in research\n\nThis project demonstrates both the elegance of classical algorithms and\ntheir inherent limitations that motivated the neural revolution.\n    \"\"\")\n\n# Main execution\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"╔\" + \"=\"*78 + \"╗\")\n    print(\"║\" + \" \"*10 + \"GRAPH-BASED DEPENDENCY PARSER - MST ALGORITHM PROJECT\" + \" \"*13 + \"║\")\n    print(\"╚\" + \"=\"*78 + \"╝\\n\")\n    \n    train_file = \"en_ewt-ud-train.conllu\"\n    dev_file = \"en_ewt-ud-dev.conllu\"\n    test_file = \"en_ewt-ud-test.conllu\"\n    \n    try:\n        # Load data\n        print(\"PHASE 1: DATA LOADING\")\n        print(\"─\"*80)\n        train_trees = CoNLLReader.read_conllu(train_file)\n        print(f\"✓ Loaded {len(train_trees)} training sentences\")\n        \n        dev_trees = CoNLLReader.read_conllu(dev_file)\n        print(f\"✓ Loaded {len(dev_trees)} development sentences\")\n        \n        test_trees = CoNLLReader.read_conllu(test_file)\n        print(f\"✓ Loaded {len(test_trees)} test sentences\")\n        \n        # Initialize parser\n        parser = MSTParser()\n        \n        # Train (use subset for demo, remove [:1000] for full training)\n        parser.train(train_trees[:1000], epochs=5, learning_rate=0.1)\n        \n        # Evaluate\n        print(\"\\nPHASE 2: EVALUATION\")\n        print(\"─\"*80)\n        print(\"\\nDevelopment Set Evaluation:\")\n        dev_uas, dev_errors = parser.evaluate(dev_trees[:200], detailed=True)\n        \n        print(\"\\nTest Set Evaluation:\")\n        test_uas, test_errors = parser.evaluate(test_trees[:200], detailed=True)\n        \n        # Save model\n        parser.save(\"mst_parser_model.pkl\")\n        \n        # Detailed testing\n        print(\"\\nPHASE 3: QUALITATIVE ANALYSIS\")\n        print(\"─\"*80)\n        test_on_diverse_examples(parser)\n        \n        # Example visualization\n        print(\"\\nPHASE 4: EXAMPLE PARSE VISUALIZATION\")\n        example_sent = [\"The\", \"dog\", \"chased\", \"the\", \"cat\", \"in\", \"the\", \"garden\"]\n        example_heads = parser.parse(example_sent)\n        visualize_parse_tree(example_sent, example_heads)\n        \n        # Generate report\n        generate_project_report(parser, train_trees[:1000], dev_trees[:200], test_trees[:200])\n        \n        print(\"\\n\" + \"╔\" + \"=\"*78 + \"╗\")\n        print(\"║\" + \" \"*25 + \"PROJECT COMPLETE\" + \" \"*36 + \"║\")\n        print(\"╚\" + \"=\"*78 + \"╝\\n\")\n        \n    except FileNotFoundError:\n        print(\"\\n❌ Error: Could not find data files.\")\n        print(\"\\nPlease run these commands first:\")\n        print(\"─\"*80)\n        print(\"!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\")\n        print(\"!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\")\n        print(\"!wget https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\")\n        print(\"─\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T13:47:22.402469Z","iopub.execute_input":"2025-12-03T13:47:22.402762Z","iopub.status.idle":"2025-12-03T13:47:42.327484Z","shell.execute_reply.started":"2025-12-03T13:47:22.402742Z","shell.execute_reply":"2025-12-03T13:47:42.326763Z"}},"outputs":[{"name":"stdout","text":"\n╔==============================================================================╗\n║          GRAPH-BASED DEPENDENCY PARSER - MST ALGORITHM PROJECT             ║\n╚==============================================================================╝\n\nPHASE 1: DATA LOADING\n────────────────────────────────────────────────────────────────────────────────\n✓ Loaded 12544 training sentences\n✓ Loaded 2001 development sentences\n✓ Loaded 2077 test sentences\n\n================================================================================\nTRAINING PHASE\n================================================================================\nTraining sentences: 1000\nEpochs: 5\nLearning rate: 0.1\nAlgorithm: Structured Perceptron with MST inference\n================================================================================\n\nEpoch 1/5 | Sentence 900/1000 | Updates: 14196\nEpoch 1/5 Complete:\n  ├─ Accuracy: 0.2971 (6493/21857)\n  ├─ Updates: 15364\n  ├─ Time: 3.72s\n  └─ Sentences/sec: 269.1\nEpoch 2/5 | Sentence 900/1000 | Updates: 12306\nEpoch 2/5 Complete:\n  ├─ Accuracy: 0.3887 (8496/21857)\n  ├─ Updates: 13361\n  ├─ Time: 3.70s\n  └─ Sentences/sec: 270.1\nEpoch 3/5 | Sentence 900/1000 | Updates: 11334\nEpoch 3/5 Complete:\n  ├─ Accuracy: 0.4387 (9589/21857)\n  ├─ Updates: 12268\n  ├─ Time: 3.51s\n  └─ Sentences/sec: 284.6\nEpoch 4/5 | Sentence 900/1000 | Updates: 10433\nEpoch 4/5 Complete:\n  ├─ Accuracy: 0.4820 (10535/21857)\n  ├─ Updates: 11322\n  ├─ Time: 3.55s\n  └─ Sentences/sec: 281.3\nEpoch 5/5 | Sentence 900/1000 | Updates: 9752\nEpoch 5/5 Complete:\n  ├─ Accuracy: 0.5166 (11291/21857)\n  ├─ Updates: 10566\n  ├─ Time: 3.45s\n  └─ Sentences/sec: 289.5\n\n================================================================================\nTRAINING COMPLETE\n================================================================================\nTotal updates: 62881\nActive features: 321628\nFinal accuracy: 0.5166\n================================================================================\n\n\nPHASE 2: EVALUATION\n────────────────────────────────────────────────────────────────────────────────\n\nDevelopment Set Evaluation:\n\nUnlabeled Attachment Score (UAS): 0.2910\nCorrect attachments: 1166/4007\n\nError Breakdown:\n  ├─ Wrong root: 138\n  ├─ False root: 65\n  └─ Wrong attachment: 2638\n\nTest Set Evaluation:\n\nUnlabeled Attachment Score (UAS): 0.2831\nCorrect attachments: 1208/4267\n\nError Breakdown:\n  ├─ Wrong root: 147\n  ├─ False root: 74\n  └─ Wrong attachment: 2838\nModel saved to mst_parser_model.pkl\n\nPHASE 3: QUALITATIVE ANALYSIS\n────────────────────────────────────────────────────────────────────────────────\n\n================================================================================\nCOMPREHENSIVE TESTING ON DIVERSE LINGUISTIC PHENOMENA\n================================================================================\n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Simple SVO\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: I saw her\nParse:\n  I               → saw            \n  saw             ← ROOT           \n  her             ← saw            \n\nSentence: Dogs bark loudly\nParse:\n  Dogs            ← ROOT           \n  bark            ← Dogs           \n  loudly          ← bark           \n\nSentence: The cat sleeps\nParse:\n  The             → sleeps         \n  cat             → sleeps         \n  sleeps          ← ROOT           \n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Prepositional Phrases (PP-Attachment)\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: I saw the man with the telescope\nParse:\n  I               → saw            \n  saw             ← ROOT           \n  the             → man            \n  man             → telescope      \n  with            ← man            \n  the             ← man            \n  telescope       ← saw            \n\nSentence: She put the book on the table\nParse:\n  She             → table          \n  put             → book           \n  the             → table          \n  book            → table          \n  on              → table          \n  the             → table          \n  table           ← ROOT           \n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Relative Clauses\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: The dog that I saw was big\nParse:\n  The             → saw            \n  dog             → saw            \n  that            → saw            \n  I               → saw            \n  saw             ← ROOT           \n  was             ← saw            \n  big             ← saw            \n\nSentence: People who live here are happy\nParse:\n  People          → happy          \n  who             → happy          \n  live            → happy          \n  here            → happy          \n  are             → happy          \n  happy           ← ROOT           \n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Coordination\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: John and Mary went home\nParse:\n  John            → went           \n  and             → went           \n  Mary            → went           \n  went            ← ROOT           \n  home            ← went           \n\nSentence: She quickly and quietly left\nParse:\n  She             → left           \n  quickly         → left           \n  and             → left           \n  quietly         → left           \n  left            ← ROOT           \n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Long-Distance Dependencies\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: What did you think he said\nParse:\n  What            → think          \n  did             → think          \n  you             → think          \n  think           → said           \n  he              ← think          \n  said            ← ROOT           \n\nSentence: Who do you believe won\nParse:\n  Who             → won            \n  do              → believe        \n  you             → won            \n  believe         → won            \n  won             ← ROOT           \n\n────────────────────────────────────────────────────────────────────────────────\nCategory: Complex Sentences\n────────────────────────────────────────────────────────────────────────────────\n\nSentence: I think that he is smart\nParse:\n  I               → think          \n  think           ← ROOT           \n  that            ← think          \n  he              ← think          \n  is              → smart          \n  smart           ← think          \n\nSentence: The horse raced past the barn fell\nParse:\n  The             → horse          \n  horse           → barn           \n  raced           ← horse          \n  past            → barn           \n  the             → fell           \n  barn            ← horse          \n  fell            ← barn           \n\n================================================================================\nKNOWN FAILURE MODES AND LIMITATIONS\n================================================================================\n\n    1. PP-ATTACHMENT AMBIGUITY\n       Example: \"I saw the man with the telescope\"\n       Challenge: Should 'telescope' attach to 'saw' (instrument) or 'man' (possession)?\n       Why it fails: Requires semantic knowledge about plausibility\n       \n    2. LONG-DISTANCE DEPENDENCIES\n       Example: \"What did you think that he said\"\n       Challenge: 'What' is the object of 'said', several words away\n       Why it fails: Non-local dependencies hard to capture with local features\n       \n    3. COORDINATION AMBIGUITY\n       Example: \"old men and women\"\n       Challenge: Does 'old' modify just 'men' or both 'men and women'?\n       Why it fails: Requires understanding of scope and semantic parallelism\n       \n    4. GARDEN PATH SENTENCES\n       Example: \"The horse raced past the barn fell\"\n       Challenge: 'raced' is initially parsed as main verb, but is actually reduced relative\n       Why it fails: No reanalysis mechanism; commits to first interpretation\n       \n    5. LIMITED LEXICAL SEMANTICS\n       Example: Semantic role assignment, verb subcategorization\n       Challenge: Different verbs have different argument structures\n       Why it fails: No access to lexical semantic databases or embeddings\n       \n    IMPROVEMENTS THAT WOULD HELP:\n    - Rich POS tags and morphological features\n    - Word embeddings or distributional semantics\n    - Syntactic category information\n    - Lexicalized grammar knowledge\n    - Larger context windows\n    - Neural scoring functions\n    \n================================================================================\n\n\n╔==============================================================================╗\n║          WHERE CLASSICAL GRAPH-BASED PARSERS LAG BEHIND                    ║\n╚==============================================================================╝\n\n================================================================================\n1. FEATURE ENGINEERING BOTTLENECK\n================================================================================\n\nClassical Approach (This Model):\n  • Manual feature templates (distance, lexical, suffix features)\n  • Sparse, high-dimensional feature vectors\n  • Human expert knowledge required\n  • Cannot capture complex feature interactions\n  • Limited to predefined feature combinations\n\nModern Neural Approach:\n  • Automatic feature learning via embeddings\n  • Dense, continuous representations\n  • Captures complex non-linear patterns\n  • Pre-trained contextualized embeddings (BERT, RoBERTa)\n  • Multi-head attention captures arbitrary feature interactions\n\nPerformance Gap: \n  • Classical MST: 60-75% UAS with basic features, 85-90% with engineered features\n  • Neural (BiLSTM): 93-95% UAS\n  • Transformer-based: 96-98% UAS\n\nReal Example Failure:\n  \"I deposited the check at the bank\"\n  Classical: Likely attaches \"bank\" to \"deposited\" (correct)\n  \n  \"I sat by the bank\"  \n  Classical: May still attach \"bank\" to \"sat\" using same features\n  ❌ Cannot distinguish financial vs. river bank without semantic context\n    \n\n================================================================================\n2. INABILITY TO CAPTURE SEMANTIC SIMILARITY\n================================================================================\n\nClassical Approach:\n  • Treats \"dog\" and \"cat\" as completely different symbols\n  • No notion of semantic relatedness\n  • Cannot generalize across similar words\n  • Sparse data problem: unseen word pairs\n\nModern Approach:\n  • Word embeddings place similar words nearby in vector space\n  • \"dog\" and \"cat\" have similar representations\n  • Generalizes patterns learned from \"dog\" to \"cat\"\n  • Pre-training on massive corpora provides semantic knowledge\n\nExample:\n  Training: \"The dog chased the cat\"\n  \n  Test: \"The wolf hunted the rabbit\"\n  \n  Classical: ❌ Treats \"wolf\" and \"hunted\" as completely novel\n  Neural: ✓ Recognizes similarity to \"dog\" and \"chased\"\n  \nImpact: \n  Classical models suffer 10-15% accuracy drop on out-of-vocabulary words\n  Neural models maintain 95%+ accuracy even with 20% OOV rate\n    \n\n================================================================================\n3. LIMITED CONTEXT WINDOW\n================================================================================\n\nClassical Approach:\n  • Features typically look at 2-3 word window\n  • Cannot capture long-range dependencies effectively\n  • Context beyond immediate neighbors is hard to encode\n  • Exponential feature explosion with larger windows\n\nModern Approach:\n  • BiLSTMs: Can access full sentence context\n  • Transformers: Self-attention over entire sequence\n  • No limit on context size\n  • Efficiently models arbitrarily long dependencies\n\nExample Failure:\n  \"The keys to the cabinet that the man who lives downstairs owns are missing\"\n  \n  Classical: Struggles to connect \"keys\" with \"are\" (10 words apart)\n  ❌ Features can't span that distance without exponential combinations\n  \n  Neural: ✓ Attention mechanism directly connects distant words\n    \n\n================================================================================\n4. GREEDY STRUCTURED PREDICTION\n================================================================================\n\nClassical MST Parser:\n  • Finds globally optimal tree given arc scores\n  • BUT arc scores are independently computed\n  • Cannot model inter-arc dependencies during scoring\n  • Structured perceptron updates are post-hoc corrections\n\nModern Approaches:\n  • Biaffine attention: Jointly scores all potential arcs\n  • Graph neural networks: Iteratively refine arc scores\n  • Structured training objectives that directly optimize tree accuracy\n  • End-to-end differentiable training\n\nProblem Example:\n  In \"John and Mary went home\", if model incorrectly scores:\n    - \"John\" → ROOT (high score)\n    - \"Mary\" → ROOT (high score)\n  \n  Classical: ❌ MST must pick one, but both have high independent scores\n  No mechanism to say \"if John is root, penalize Mary being root\"\n  \n  Neural: ✓ Can learn that coordinated subjects shouldn't both be roots\n    \n\n================================================================================\n5. NO TRANSFER LEARNING OR PRE-TRAINING\n================================================================================\n\nClassical Approach:\n  • Trained from scratch on labeled dependency data\n  • Requires large annotated treebanks (10,000+ sentences)\n  • Cannot leverage unlabeled text\n  • Separate models for each language\n\nModern Approach:\n  • Pre-train on billions of tokens of unlabeled text\n  • Fine-tune on small labeled datasets (1,000 sentences can suffice)\n  • Transfer learning across languages (multilingual BERT)\n  • Can leverage knowledge from related tasks\n\nImpact on Low-Resource Languages:\n  Classical: Needs 10K+ sentences → 75% UAS\n  Neural (from scratch): Same requirement → 85% UAS\n  Neural (pre-trained): Just 1K sentences → 90% UAS\n  Neural (multilingual): Even works zero-shot! → 75-80% UAS\n    \n\n================================================================================\n6. FIXED FEATURE REPRESENTATIONS\n================================================================================\n\nClassical Approach:\n  • Word \"bank\" always has same features\n  • Cannot handle polysemy (words with multiple meanings)\n  • Context-independent representation\n  • Ambiguity must be resolved by parser, not representation\n\nModern Contextualized Embeddings:\n  • \"bank\" has different representations in different contexts\n  • BERT/ELMo create context-dependent embeddings\n  • Ambiguity partially resolved before parsing\n  • Richer input to parser\n\nExample:\n  \"I went to the bank to deposit money\"\n  \"I sat by the river bank\"\n  \n  Classical: \"bank\" → same features in both\n  ❌ Parser must figure out the difference from limited context features\n  \n  BERT: \"bank\" → different embeddings (financial institution vs. riverbank)\n  ✓ Pre-resolved ambiguity helps parser make correct attachment\n    \n\n================================================================================\n7. TRAINING DATA EFFICIENCY\n================================================================================\n\nClassical Perceptron:\n  • Requires many passes over data (5-20 epochs)\n  • Slow convergence on complex patterns\n  • Prone to overfitting on rare features\n  • Hand-crafted features may not capture patterns in data\n\nModern Neural Networks:\n  • More sample efficient with pre-training\n  • Gradient-based optimization converges faster\n  • Regularization techniques (dropout, batch norm) prevent overfitting\n  • Learns optimal features from data\n\nTraining Time Comparison (to 90% UAS):\n  Classical MST: 10K sentences × 15 epochs = 150K sentence-epochs\n  Neural (scratch): 10K sentences × 30 epochs = 300K sentence-epochs\n  Neural (pre-trained): 2K sentences × 10 epochs = 20K sentence-epochs\n    \n\n================================================================================\n8. HANDLING OF RARE PHENOMENA\n================================================================================\n\nClassical Sparse Features:\n  • Rare word pairs get few training examples\n  • Model cannot generalize from similar contexts\n  • Effectively backs off to simple heuristics\n  • Example: Rare verbs parsed using generic \"verb\" patterns\n\nNeural Dense Representations:\n  • Rare words still have meaningful embeddings\n  • Similar words provide indirect evidence\n  • Continuous space allows interpolation\n  • Compositional understanding helps with novel constructions\n\nExample:\n  Rare verb \"scrutinize\" appears once in training\n  \n  Classical: ❌ Creates features like \"scrutinize_OBJ\" with single training example\n  Cannot generalize\n  \n  Neural: ✓ \"scrutinize\" embedding is close to \"examine\", \"inspect\"\n  Learns from all similar verbs → better generalization\n    \n\n================================================================================\n9. CROSS-LINGUAL PARSING\n================================================================================\n\nClassical Approach:\n  • Completely separate models per language\n  • Features like word forms are language-specific\n  • No knowledge transfer between languages\n  • Need large treebank for each new language\n\nModern Multilingual Models:\n  • Single model handles 100+ languages\n  • Shared multilingual embeddings (mBERT, XLM-R)\n  • Zero-shot transfer: Train on one language, parse another\n  • Low-resource languages benefit from high-resource ones\n\nZero-Shot Performance:\n  Train on English, test on Spanish:\n    Classical: 0% (no Spanish vocabulary)\n    Multilingual BERT: 75-80% UAS\n    \n  This is revolutionary for low-resource languages!\n    \n\n================================================================================\n10. PRACTICAL DEPLOYMENT CONSIDERATIONS\n================================================================================\n\nClassical Model Advantages:\n  ✓ Fast inference (MST algorithm is O(n²))\n  ✓ Small model size (few MB)\n  ✓ Interpretable features\n  ✓ No GPU required\n  ✓ Deterministic output\n\nClassical Model Disadvantages:\n  ❌ 10-15% lower accuracy\n  ❌ Requires linguistic expertise for feature engineering\n  ❌ Hard to improve beyond certain accuracy ceiling\n  ❌ Separate preprocessing pipeline (POS tagging, etc.)\n\nModern Neural Model Tradeoffs:\n  ✓ State-of-the-art accuracy\n  ✓ End-to-end training\n  ✓ Easily adaptable to new domains\n  \n  ❌ Slower inference (especially transformers)\n  ❌ Large model size (100MB - 1GB)\n  ❌ Requires GPU for practical speed\n  ❌ Black box (hard to interpret)\n\nUse Cases:\n  Classical: Educational, resource-constrained, need interpretability\n  Neural: Production systems where accuracy is critical\n    \n\n================================================================================\nSUMMARY: PERFORMANCE COMPARISON\n================================================================================\n\nModel Type                           UAS (English)    Speed       Model Size\n─────────────────────────────────────────────────────────────────────────────\nClassical MST (basic features)          65-75%        1000 sent/s     5 MB\nClassical MST (engineered features)     85-90%        500 sent/s      50 MB\nNeural BiLSTM (no pre-train)            91-93%        100 sent/s      100 MB\nNeural BiLSTM + BiAffine                93-95%        80 sent/s       150 MB\nBERT-based (pre-trained)                96-98%        20 sent/s       400 MB\nCurrent SOTA (2024)                     98-99%        10 sent/s       1 GB\n\nKey Insight:\nThe gap from 90% → 98% accuracy required fundamentally different approaches:\n  • Continuous representations instead of sparse features\n  • Neural composition instead of manual feature templates  \n  • Pre-training instead of training from scratch\n  • End-to-end learning instead of pipeline systems\n    \n\n╔==============================================================================╗\n║               CONCLUSION: WHY NEURAL PARSERS DOMINATE                       ║\n╚==============================================================================╝\n\n\nClassical graph-based parsers were a major advance in the 2000s, but they\nfundamentally cannot compete with modern neural approaches because:\n\n1. They cannot learn semantic representations from data\n2. They require manual feature engineering by experts\n3. They cannot efficiently use unlabeled data\n4. They cannot transfer knowledge across languages or domains\n5. They plateau at ~90% accuracy regardless of data size\n\nNeural parsers overcome ALL of these limitations, achieving near-human\nperformance (98-99% UAS) on standard benchmarks.\n\nHowever, classical parsers remain valuable for:\n  • Understanding the foundations of dependency parsing\n  • Educational purposes\n  • Resource-constrained environments\n  • Interpretable systems where you need to explain decisions\n  • Baseline comparisons in research\n\nThis project demonstrates both the elegance of classical algorithms and\ntheir inherent limitations that motivated the neural revolution.\n    \n\nPHASE 4: EXAMPLE PARSE VISUALIZATION\n\n================================================================================\nDEPENDENCY TREE VISUALIZATION\n================================================================================\n\nTokens:\n  [1] The\n  [2] dog\n  [3] chased\n  [4] the\n  [5] cat\n  [6] in\n  [7] the\n  [8] garden\n\nDependency Arcs:\n--------------------------------------------------------------------------------\nDependent       Head            Relation        Direction \n--------------------------------------------------------------------------------\nThe             cat             dep             →         \ndog             garden          dep             →         \nchased          cat             dep             →         \nthe             garden          dep             →         \ncat             garden          dep             →         \nin              garden          dep             →         \nthe             garden          dep             →         \ngarden          ROOT            dep             ←         \n================================================================================\n\n\n╔==============================================================================╗\n║               GRAPH-BASED DEPENDENCY PARSING PROJECT REPORT                ║\n╚==============================================================================╝\n\nProject Date: 2025-12-03 13:47:42\nAlgorithm: Maximum Spanning Tree (Chu-Liu-Edmonds)\nTraining Method: Structured Perceptron\n\n────────────────────────────────────────────────────────────────────────────────\nDATASET STATISTICS\n────────────────────────────────────────────────────────────────────────────────\nTraining sentences: 1000\nDevelopment sentences: 200\nTest sentences: 200\nTraining tokens: 21857\nAverage sentence length: 21.9 tokens\n\nMost common dependency relations:\n  punct             2550 (11.7%)\n  case              2307 (10.6%)\n  det               1894 (8.7%)\n  nsubj             1490 (6.8%)\n  amod              1393 (6.4%)\n  advmod            1088 (5.0%)\n  obl               1013 (4.6%)\n  nmod              1008 (4.6%)\n  root              1000 (4.6%)\n  obj                898 (4.1%)\n\n────────────────────────────────────────────────────────────────────────────────\nMODEL STATISTICS\n────────────────────────────────────────────────────────────────────────────────\nTotal features learned: 416211\nTotal weight updates: 62881\nTraining epochs: 5\n\n================================================================================\nFEATURE ANALYSIS\n================================================================================\n\nTop 15 Most Important Features:\n--------------------------------------------------------------------------------\nFeature                                                     Weight      Count\n--------------------------------------------------------------------------------\nhead=.                                                     -4.4000         44\nhead=,                                                     -3.5000         35\nhead_dep=root_said                                          3.2000         64\nhead=\"                                                     -3.0000         30\nhead=in                                                    -2.9000         39\nhead=and                                                   -2.5000        125\nhead_dep=sri_lanka                                          2.4000         26\nhead=pulled                                                -2.4000         84\nhead=inspired                                              -2.4000         32\nhead_dep=root_,                                            -2.3000         23\nhead=to                                                    -2.3000         43\nhead_dep=india_'s                                           2.3000         41\nhead=by                                                    -2.2000         22\nhead=an                                                    -2.2000         22\nhead_dep=al_qaeda                                           2.2000         26\n\n================================================================================\n\n\n================================================================================\n\n\n╔==============================================================================╗\n║                         PROJECT COMPLETE                                    ║\n╚==============================================================================╝\n\n","output_type":"stream"}],"execution_count":4}]}